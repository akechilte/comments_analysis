{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model and make prediction\n",
    "##### Input Args:\n",
    "\n",
    "doc_dir = directory where the file with comments to be predicted\n",
    "\n",
    "doc2vec_model_name = doc2vec model already trained\n",
    "\n",
    "trained_clf = Trained classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_words(sent):\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub(r'<[^>]+>', ' ', sent) #strip html tags\n",
    "    sent = re.sub(r'(\\w)\\'(\\w)', '\\1\\2', sent) # remoce apostrophes\n",
    "    sent = re.sub(r'\\W',' ', sent) # remove punctuation\n",
    "    sent = re.sub(r'\\s+', ' ', sent) # remove repeated space\n",
    "    sent = sent.strip()\n",
    "    return sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load doc to vec model\n",
    "doc2vec_model_name = 'reviews.d2v'\n",
    "doc2vec_model = Doc2Vec.load(doc2vec_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7315774]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test the trained doc2vec model\n",
    "cosine_similarity(\n",
    "[doc2vec_model.infer_vector(extract_words(\"This is very bad video. I don't like it\"))],\n",
    "[doc2vec_model.infer_vector(extract_words(\"video sucks.\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converts the comments to vector using doc2vec model trianed earlier\n",
    "def get_doc2vec(model, doc_dir,comment_text_pos=0):\n",
    "    sentences = []\n",
    "    sentvecs = []\n",
    "    \n",
    "    for fname in sorted(os.listdir(doc_dir)):\n",
    "        with open(doc_dir + \"/\" + fname, encoding='UTF-8') as f:\n",
    "            print(\"files being read : {0}\".format(fname))\n",
    "            for i, line in enumerate(f):\n",
    "                line_split = line.strip().split('\\t')\n",
    "                sentences.append(line_split[comment_text_pos])\n",
    "                words = extract_words(line_split[comment_text_pos])\n",
    "                sentvecs.append(model.infer_vector(words, steps=10)) # create a vector for this document\n",
    "        \n",
    "    #sentences, sentvecs together\n",
    "    return sentences, sentvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files being read : yelp_labelled.txt\n"
     ]
    }
   ],
   "source": [
    "doc_dir = \"data/Test\"\n",
    "sentences, sentvecs = get_doc2vec(doc2vec_model, doc_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences : 1000\n"
     ]
    }
   ],
   "source": [
    "print(\"sentences : {0}\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wow... Loved this place.',\n",
       " 'Crust is not good.',\n",
       " 'Not tasty and the texture was just nasty.',\n",
       " 'Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.',\n",
       " 'The selection on the menu was great and so were the prices.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.2580039 ,  0.17749394,  0.16608469, -0.04063711, -0.23091166,\n",
       "        -0.21782547,  0.18073712, -0.4600048 ,  0.20689465,  0.3588468 ,\n",
       "         0.17917204, -0.17277159, -0.05354812,  0.01287622, -0.03258529,\n",
       "         0.01657632,  0.04913678,  0.05231582, -0.17856082,  0.1171497 ,\n",
       "        -0.24506575, -0.17292798, -0.21647501,  0.01017446,  0.17202368,\n",
       "        -0.01229254,  0.08021051,  0.27074   , -0.21875386,  0.03605529,\n",
       "         0.24077706, -0.1612347 ,  0.16099921,  0.03200743,  0.02535386,\n",
       "        -0.13572364,  0.05821555, -0.05677211,  0.26938176,  0.12989739,\n",
       "        -0.07208392, -0.08137994, -0.01896521, -0.0694478 ,  0.27666095,\n",
       "         0.06179665,  0.14333087,  0.09620525, -0.33399186,  0.05078559],\n",
       "       dtype=float32),\n",
       " array([-0.00849153,  0.15919824,  0.01353165, -0.00815025,  0.06872298,\n",
       "        -0.16078253,  0.06390469, -0.07502481,  0.0244438 ,  0.02509074,\n",
       "        -0.09995453, -0.11003245, -0.15937136,  0.21890596,  0.01556243,\n",
       "        -0.07222971, -0.01089211,  0.11592992,  0.0269406 ,  0.2301624 ,\n",
       "        -0.19811498, -0.00357268,  0.04939818, -0.24688007, -0.07350828,\n",
       "         0.26147163,  0.18861552,  0.08459561, -0.0898061 ,  0.09632067,\n",
       "         0.33428934,  0.10125466,  0.1304533 ,  0.10217002, -0.09672592,\n",
       "         0.04811995,  0.12167768, -0.12456272,  0.21445236, -0.15903544,\n",
       "         0.05414785, -0.11345383, -0.04871579, -0.20699388,  0.19935893,\n",
       "        -0.15536448,  0.12228894, -0.00517921, -0.07973584,  0.41060707],\n",
       "       dtype=float32),\n",
       " array([ 0.10490934, -0.04056428,  0.03419401,  0.00226079, -0.1709096 ,\n",
       "        -0.06871131,  0.02167132, -0.2088958 ,  0.0310897 , -0.07167386,\n",
       "        -0.06878667,  0.15373893,  0.02061422,  0.35018682,  0.40906644,\n",
       "        -0.08364981, -0.21079406,  0.09594139,  0.05600617,  0.10839058,\n",
       "        -0.17144686, -0.20898691,  0.04146213,  0.02625287, -0.00702266,\n",
       "         0.09269834,  0.02223936,  0.29823697, -0.5053069 ,  0.23650584,\n",
       "         0.20305382, -0.03937929,  0.07275022, -0.06983647, -0.05181391,\n",
       "        -0.06634675, -0.04148053, -0.37102127,  0.17705975, -0.07863492,\n",
       "        -0.18628427, -0.39567122,  0.07377117, -0.13118194,  0.37773693,\n",
       "        -0.0420242 ,  0.3964314 , -0.097759  , -0.13599828,  0.08719879],\n",
       "       dtype=float32),\n",
       " array([ 0.54097575,  0.21740673,  0.10606764, -0.10717261, -0.17739686,\n",
       "        -0.03872579,  0.29528728, -0.77126795,  0.02162196,  0.27147385,\n",
       "        -0.3604252 , -0.28750116, -0.16071062, -0.15567116,  0.17433578,\n",
       "        -0.19740537, -0.10001411,  0.07564536, -0.17531264,  0.24081625,\n",
       "        -0.193883  , -0.29568538, -0.06594007, -0.34195647,  0.43171987,\n",
       "         0.12191156, -0.11533181,  0.27816227, -0.18830788,  0.40575892,\n",
       "         0.11299741,  0.14959951,  0.2525339 , -0.01978072, -0.49715364,\n",
       "         0.47093883, -0.08661568, -0.09027165,  0.62261826,  0.20919843,\n",
       "        -0.35371664, -0.10915823,  0.08379855,  0.0663035 , -0.19541778,\n",
       "        -0.8362366 ,  0.33328772,  0.18615144,  0.31330955,  0.2839886 ],\n",
       "       dtype=float32),\n",
       " array([ 0.25198674,  0.06253373,  0.11310025, -0.18278757, -0.08441486,\n",
       "         0.18438019,  0.33645543, -0.4924302 , -0.07067975,  0.2753179 ,\n",
       "        -0.26329997, -0.42230895, -0.19274448, -0.21252419,  0.12849356,\n",
       "        -0.20424214,  0.27753055,  0.17185219,  0.13252805,  0.0383852 ,\n",
       "        -0.15792525, -0.38327065,  0.03399839, -0.06737983, -0.21767418,\n",
       "        -0.28352833,  0.24950862,  0.40102664, -0.09864189,  0.33550087,\n",
       "         0.14460103, -0.12650533,  0.3792872 , -0.1804249 , -0.02478407,\n",
       "         0.00053147,  0.11333893, -0.3946973 ,  0.33874324, -0.2430215 ,\n",
       "         0.00907769, -0.46528384, -0.05809677, -0.07627815,  0.0867945 ,\n",
       "        -0.14408736,  0.41901022, -0.07501151, -0.18704355,  0.12596206],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentvecs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_clf = 'doc2vec_spam_rf.model'\n",
    "loaded_clf = joblib.load(trained_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0\n",
      " 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0\n",
      " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0\n",
      " 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1\n",
      " 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "y_pred = loaded_clf.predict(sentvecs)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trained_clf = 'doc2vec_spam_kn.model'\n",
    "loaded_clf = joblib.load(trained_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 0\n",
      " 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0\n",
      " 0 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1\n",
      " 0 1 1 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1\n",
      " 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1\n",
      " 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0\n",
      " 0 0 0 1 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 0\n",
      " 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1\n",
      " 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1\n",
      " 0 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 1 1\n",
      " 0 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1\n",
      " 1 1 0 0 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0\n",
      " 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0\n",
      " 0 1 1 0 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 1\n",
      " 0 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 0\n",
      " 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 0\n",
      " 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 1 0 1 1\n",
      " 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0\n",
      " 1 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 1\n",
      " 1 0 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 1 1 1 0 0 0\n",
      " 1 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1\n",
      " 0 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 0\n",
      " 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "y_pred = loaded_clf.predict(sentvecs)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
