{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_words(sent):\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub(r'<[^>]+>', ' ', sent) #strip html tags\n",
    "    sent = re.sub(r'(\\w)\\'(\\w)', '\\1\\2', sent) # remoce apostrophes\n",
    "    sent = re.sub(r'\\W',' ', sent) # remove punctuation\n",
    "    sent = re.sub(r'\\s+', ' ', sent) # remove repeated space\n",
    "    sent = sent.strip()\n",
    "    return sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ajshd', 'asda', '1231', 'sd', 'alkdj']\n"
     ]
    }
   ],
   "source": [
    "# test text cleanup block\n",
    "words = 'ajshd asda, + 1231 \"sd + ''alkdj''    '\n",
    "print(extract_words(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unsup_sentences = []\n",
    "\n",
    "# data \n",
    "\"\"\"\n",
    "https://ai.stanford.edu/~amaas/data/sentiment/, IMDB data\n",
    "http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "https://nlp.stanford.edu/sentiment/, Rotten Tomatoes data\n",
    "\n",
    "Sentiment:\n",
    "https://www.kaggle.com/rahulin05/sentiment-labelled-sentences-data-set?login=true\n",
    "\"\"\"\n",
    "\n",
    "# https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "# Document tag is important in way that it shows all these words go together in one document\n",
    "# There is no label in this case. We are trying to understand which words are used together\n",
    "for dirname in [\"train/pos\", \"train/neg\", \"train/unsup\", \"test/pos\", \"test/neg\"]:\n",
    "    for fname in sorted(os.listdir(\"data/aclImdb/\" + dirname)):\n",
    "        if fname[-4:] == '.txt':\n",
    "            with open(\"data/aclImdb/\" + dirname + '/' + fname, encoding = 'UTF-8') as f:\n",
    "                sent = f.read()\n",
    "                words = extract_words(sent)\n",
    "                unsup_sentences.append(TaggedDocument(words,[dirname + \"/\" + fname]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsup_sentences length: 100000\n",
      "unsup_sentences length: TaggedDocument(['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', 'such', 'as', 'teachers', 'my', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'bromwell', 'hig', 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', 'teachers', 'the', 'scramble', 'to', 'survive', 'financially', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', 'pomp', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', 'i', 'immediately', 'recalled', 'at', 'high', 'a', 'classic', 'line', 'inspector', 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', 'student', 'welcome', 'to', 'bromwell', 'high', 'i', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'bromwell', 'high', 'is', 'far', 'fetched', 'what', 'a', 'pity', 'that', 'it', 'is'], ['train/pos/0_9.txt'])\n"
     ]
    }
   ],
   "source": [
    "print(\"unsup_sentences length: {0}\".format(len(unsup_sentences)))\n",
    "print(\"unsup_sentences length: {0}\".format(unsup_sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "for dirname in [\"data/review_polarity/txt_sentoken/pos\", \"data/review_polarity/txt_sentoken/neg\"]:\n",
    "    for fname in sorted(os.listdir(dirname)):\n",
    "        if fname[-4:] == '.txt':\n",
    "            with open(dirname + '/' + fname, encoding = 'UTF-8') as f:\n",
    "                for i, sent in enumerate(f):\n",
    "                    words = extract_words(sent)\n",
    "                    unsup_sentences.append(TaggedDocument(words,[\"%s/%s-%d\" % (dirname, fname, i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://nlp.stanford.edu/sentiment/\n",
    "with open(\"data/stanfordSentimentTreebank/original_rt_snippets.txt\", encoding='UTF-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        words = extract_words(sent)\n",
    "        unsup_sentences.append(TaggedDocument(words, [\"rt-%d\" % i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175325"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unsup_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', 'such', 'as', 'teachers', 'my', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'bromwell', 'hig', 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', 'teachers', 'the', 'scramble', 'to', 'survive', 'financially', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', 'pomp', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', 'i', 'immediately', 'recalled', 'at', 'high', 'a', 'classic', 'line', 'inspector', 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', 'student', 'welcome', 'to', 'bromwell', 'high', 'i', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'bromwell', 'high', 'is', 'far', 'fetched', 'what', 'a', 'pity', 'that', 'it', 'is'], tags=['train/pos/0_9.txt']),\n",
       " TaggedDocument(words=['homelessness', 'or', 'houselessness', 'as', 'george', 'carlin', 'stated', 'has', 'been', 'an', 'issue', 'for', 'years', 'but', 'never', 'a', 'plan', 'to', 'help', 'those', 'on', 'the', 'street', 'that', 'were', 'once', 'considered', 'human', 'who', 'did', 'everything', 'from', 'going', 'to', 'school', 'work', 'or', 'vote', 'for', 'the', 'matter', 'most', 'people', 'think', 'of', 'the', 'homeless', 'as', 'just', 'a', 'lost', 'cause', 'while', 'worrying', 'about', 'things', 'such', 'as', 'racism', 'the', 'war', 'on', 'iraq', 'pressuring', 'kids', 'to', 'succeed', 'technology', 'the', 'elections', 'inflation', 'or', 'worrying', 'if', 'the', 'l', 'be', 'next', 'to', 'end', 'up', 'on', 'the', 'streets', 'but', 'what', 'if', 'you', 'were', 'given', 'a', 'bet', 'to', 'live', 'on', 'the', 'streets', 'for', 'a', 'month', 'without', 'the', 'luxuries', 'you', 'once', 'had', 'from', 'a', 'home', 'the', 'entertainment', 'sets', 'a', 'bathroom', 'pictures', 'on', 'the', 'wall', 'a', 'computer', 'and', 'everything', 'you', 'once', 'treasure', 'to', 'see', 'what', 'i', 'like', 'to', 'be', 'homeless', 'that', 'is', 'goddard', 'bol', 'lesson', 'mel', 'brooks', 'who', 'directs', 'who', 'stars', 'as', 'bolt', 'plays', 'a', 'rich', 'man', 'who', 'has', 'everything', 'in', 'the', 'world', 'until', 'deciding', 'to', 'make', 'a', 'bet', 'with', 'a', 'sissy', 'rival', 'jeffery', 'tambor', 'to', 'see', 'if', 'he', 'can', 'live', 'in', 'the', 'streets', 'for', 'thirty', 'days', 'without', 'the', 'luxuries', 'if', 'bolt', 'succeeds', 'he', 'can', 'do', 'what', 'he', 'wants', 'with', 'a', 'future', 'project', 'of', 'making', 'more', 'buildings', 'the', 'be', 'on', 'where', 'bolt', 'is', 'thrown', 'on', 'the', 'street', 'with', 'a', 'bracelet', 'on', 'his', 'leg', 'to', 'monitor', 'his', 'every', 'move', 'where', 'he', 'ca', 'step', 'off', 'the', 'sidewalk', 'h', 'given', 'the', 'nickname', 'pepto', 'by', 'a', 'vagrant', 'after', 'i', 'written', 'on', 'his', 'forehead', 'where', 'bolt', 'meets', 'other', 'characters', 'including', 'a', 'woman', 'by', 'the', 'name', 'of', 'molly', 'lesley', 'ann', 'warren', 'an', 'ex', 'dancer', 'who', 'got', 'divorce', 'before', 'losing', 'her', 'home', 'and', 'her', 'pals', 'sailor', 'howard', 'morris', 'and', 'fumes', 'teddy', 'wilson', 'who', 'are', 'already', 'used', 'to', 'the', 'streets', 'the', 'e', 'survivors', 'bolt', 'is', 'h', 'not', 'used', 'to', 'reaching', 'mutual', 'agreements', 'like', 'he', 'once', 'did', 'when', 'being', 'rich', 'where', 'i', 'fight', 'or', 'flight', 'kill', 'or', 'be', 'killed', 'while', 'the', 'love', 'connection', 'between', 'molly', 'and', 'bolt', 'was', 'necessary', 'to', 'plot', 'i', 'found', 'life', 'stinks', 'to', 'be', 'one', 'of', 'mel', 'brooks', 'observant', 'films', 'where', 'prior', 'to', 'being', 'a', 'comedy', 'it', 'shows', 'a', 'tender', 'side', 'compared', 'to', 'his', 'slapstick', 'work', 'such', 'as', 'blazing', 'saddles', 'young', 'frankenstein', 'or', 'spaceballs', 'for', 'the', 'matter', 'to', 'show', 'what', 'i', 'like', 'having', 'something', 'valuable', 'before', 'losing', 'it', 'the', 'next', 'day', 'or', 'on', 'the', 'other', 'hand', 'making', 'a', 'stupid', 'bet', 'like', 'all', 'rich', 'people', 'do', 'when', 'they', 'do', 'know', 'what', 'to', 'do', 'with', 'their', 'money', 'maybe', 'they', 'should', 'give', 'it', 'to', 'the', 'homeless', 'instead', 'of', 'using', 'it', 'like', 'monopoly', 'money', 'or', 'maybe', 'this', 'film', 'will', 'inspire', 'you', 'to', 'help', 'others'], tags=['train/pos/10000_8.txt'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsup_sentences[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PermuteSentences(object):\n",
    "    def __init__(self, sents):\n",
    "        self.sents = sents\n",
    "    def __iter__(self):\n",
    "        shuffled = list(self.sents)\n",
    "        random.shuffle(shuffled)\n",
    "        for sent in shuffled:\n",
    "            yield sent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "permuter = PermuteSentences(unsup_sentences)\n",
    "#Training of Doc2Vec model\n",
    "model = Doc2Vec(permuter, dm=0, hs=1, vector_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# done with training, free up some memory\n",
    "model.delete_temporary_training_data(keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('reviews.d2v')\n",
    "# in other program, we could write: model = Doc2Vec.load('reviews.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.infer_vector(extract_words(\"This is very bad video. I don't like it\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(\n",
    "[model.infer_vector(extract_words(\"This is very bad video. I don't like it\"))],\n",
    "[model.infer_vector(extract_words(\"video sucks.\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(\n",
    "[model.infer_vector(extract_words(\"It is now snowing in New York\"))],\n",
    "[model.infer_vector(extract_words(\"I feel sick. Dont feel like going to school\"))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
