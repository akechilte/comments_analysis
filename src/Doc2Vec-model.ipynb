{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_words(sent):\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub(r'<[^>]+>', ' ', sent) #strip html tags\n",
    "    sent = re.sub(r'(\\w)\\'(\\w)', '\\1\\2', sent) # remoce apostrophes\n",
    "    sent = re.sub(r'\\W',' ', sent) # remove punctuation\n",
    "    sent = re.sub(r'\\s+', ' ', sent) # remove repeated space\n",
    "    sent = sent.strip()\n",
    "    return sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'wish', 'i', 'could', 'fly']\n"
     ]
    }
   ],
   "source": [
    "# test text cleanup block\n",
    "words = 'I wish + I could fly...-''    '\n",
    "print(extract_words(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working dir : /Users/dbiswas/Documents/Malabika/MS/Fall2018/social_media_mining/project/comments_analysis\n"
     ]
    }
   ],
   "source": [
    "proj_root = os.path.dirname(os.getcwd())\n",
    "#print(\"Current working dir : {0}\".format(proj_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "imdb_subdir = 'traindata/imdb'\n",
    "imdb_dir = proj_root + \"/\" + imdb_subdir\n",
    "\n",
    "# Document tag is important in way that it shows all these words go together in one document .There is no label in \n",
    "# this case. We are trying to understand which words are used together\n",
    "for dirname in [\"positive\", \"negative\"]:\n",
    "    for fname in sorted(os.listdir(imdb_dir + \"/\" + dirname)):\n",
    "        if fname.endswith('.txt'):\n",
    "            with open(imdb_dir + \"/\" + dirname + '/' + fname, encoding = 'UTF-8') as f:\n",
    "                sent = f.read()\n",
    "                words = extract_words(sent)\n",
    "                sentences.append(TaggedDocument(words,[dirname + \"/\" + fname]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences length: 7\n",
      "sentences length: TaggedDocument(['oh', 'the', 'sixties', 'there', 'were', 'some', 'interesting', 'films', 'i', 'was', 'more', 'of', 'a', 'movie', 'goer', 'then', 'i', 'now', 'enjoy', 'renting', 'movies', 'and', 'relaxing', 'in', 'my', 'home', 'rather', 'than', 'going', 'to', 'the', 'theater', 'i', 'also', 'saw', 'this', 'short', 'film', 'the', 'legend', 'of', 'the', 'boy', 'and', 'the', 'eagle', 'i', 'have', 'been', 'searching', 'for', 'this', 'film', 'for', 'years', 'it', 'was', 'truly', 'inspiring', 'surprisingly', 'i', 'was', 'finally', 'able', 'to', 'gather', 'more', 'information', 'from', 'your', 'site', 'thank', 'you', 'surprised', 'to', 'find', 'out', 'that', 'this', 'short', 'film', 'was', 'an', 'opening', 'for', 'a', 'disney', 'picture', 'i', 'too', 'did', 'not', 'remember', 'the', 'disney', 'film', 'i', 'did', 'not', 'even', 'remember', 'that', 'it', 'was', 'an', 'opening', 'film', 'for', 'disney', 'i', 'truly', 'wish', 'they', 'would', 'show', 'this', 'on', 'tv', 'sometime', 'i', 'wonder', 'if', 'disey', 'holds', 'the', 'rights', 'to', 'this', 'film', 'is', 'it', 'available', 'on', 'dvd', 'this', 'is', 'a', 'must', 'see', 'for', 'all', 'generations'], ['positive/11590_10.txt'])\n"
     ]
    }
   ],
   "source": [
    "print(\"sentences length: {0}\".format(len(sentences)))\n",
    "print(\"sentences length: {0}\".format(sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_subdir = 'traindata/review'\n",
    "review_dir = proj_root + \"/\" + review_subdir\n",
    "\n",
    "for dirname in [\"positive\", \"negative\"]:\n",
    "    for fname in sorted(os.listdir(review_dir + \"/\" + dirname)):\n",
    "        if fname.endswith('.txt'):\n",
    "            with open(review_dir + \"/\" + dirname + \"/\" + fname, encoding = 'UTF-8') as f:\n",
    "                for i, sent in enumerate(f):\n",
    "                    words = extract_words(sent)\n",
    "                    sentences.append(TaggedDocument(words,[\"%s/%s-%d\" % (dirname, fname, i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanford_subdir = 'traindata/stanford'\n",
    "stanford_dir = proj_root + \"/\" + stanford_subdir\n",
    "\n",
    "with open(stanford_dir + \"/\" + \"original_rt_snippets.txt\", encoding='UTF-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        words = extract_words(sent)\n",
    "        sentences.append(TaggedDocument(words, [\"rt-%d\" % i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1507489"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"Number of training examples : {0}\".format(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['oh', 'the', 'sixties', 'there', 'were', 'some', 'interesting', 'films', 'i', 'was', 'more', 'of', 'a', 'movie', 'goer', 'then', 'i', 'now', 'enjoy', 'renting', 'movies', 'and', 'relaxing', 'in', 'my', 'home', 'rather', 'than', 'going', 'to', 'the', 'theater', 'i', 'also', 'saw', 'this', 'short', 'film', 'the', 'legend', 'of', 'the', 'boy', 'and', 'the', 'eagle', 'i', 'have', 'been', 'searching', 'for', 'this', 'film', 'for', 'years', 'it', 'was', 'truly', 'inspiring', 'surprisingly', 'i', 'was', 'finally', 'able', 'to', 'gather', 'more', 'information', 'from', 'your', 'site', 'thank', 'you', 'surprised', 'to', 'find', 'out', 'that', 'this', 'short', 'film', 'was', 'an', 'opening', 'for', 'a', 'disney', 'picture', 'i', 'too', 'did', 'not', 'remember', 'the', 'disney', 'film', 'i', 'did', 'not', 'even', 'remember', 'that', 'it', 'was', 'an', 'opening', 'film', 'for', 'disney', 'i', 'truly', 'wish', 'they', 'would', 'show', 'this', 'on', 'tv', 'sometime', 'i', 'wonder', 'if', 'disey', 'holds', 'the', 'rights', 'to', 'this', 'film', 'is', 'it', 'available', 'on', 'dvd', 'this', 'is', 'a', 'must', 'see', 'for', 'all', 'generations'], tags=['positive/11590_10.txt']),\n",
       " TaggedDocument(words=['an', 'unusual', 'take', 'on', 'time', 'travel', 'instead', 'of', 'traveling', 'to', 'eart', 'past', 'the', 'main', 'trio', 'get', 'stuck', 'in', 'the', 'past', 'history', 'of', 'another', 'planet', 'they', 'beam', 'down', 'to', 'this', 'planet', 'whose', 'sun', 'is', 'scheduled', 'to', 'go', 'nova', 'in', '3', 'or', '4', 'hours', 'tha', 'cutting', 'it', 'close', 'in', 'some', 'kind', 'of', 'futuristic', 'library', 'they', 'meet', 'mr', 'atoz', 'a', 'to', 'z', 'get', 'it', 'ha', 'ha', 'and', 'his', 'duplicates', 'it', 'turns', 'out', 'instead', 'of', 'escaping', 'their', 'plane', 'destruction', 'via', 'space', 'travel', 'the', 'usual', 'way', 'the', 'inhabitants', 'have', 'all', 'escaped', 'into', 'their', 'plane', 'various', 'past', 'time', 'eras', 'mr', 'atoz', 'uses', 'a', 'time', 'machine', 'to', 'send', 'people', 'on', 'their', 'way', 'after', 'they', 'make', 'a', 'selection', 'check', 'out', 'the', 'discs', 'we', 'see', 'here', 'another', 'trek', 'prognostication', 'of', 'cds', 'and', 'dvds', 'when', 'mr', 'atoz', 'prepares', 'the', 'machine', 'the', 'atavachron', 'what', 'sis', 'gallant', 'kirk', 'hears', 'a', 'woma', 'scream', 'and', 'runs', 'into', 'the', 'plane', 'version', 'of', 'eart', '17th', 'century', 'where', 'he', 'gets', 'into', 'a', 'sword', 'fight', 'and', 'is', 'arrested', 'for', 'witchery', 'ther', 'an', 'eccentric', 'but', 'good', 'performance', 'here', 'by', 'the', 'actress', 'playing', 'a', 'female', 'of', 'ill', 'repute', 'in', 'this', 'time', 'using', 'phrasing', 'of', 'the', 'time', 'yo', 'e', 'a', 'bully', 'fine', 'coo', 'witch', 'witch', 'the', 'l', 'burn', 'ye', 'spock', 'mccoy', 'follow', 'kirk', 'but', 'end', 'up', 'in', 'an', 'ice', 'age', '5000', 'years', 'earlier', 'kirk', 'manages', 'to', 'get', 'back', 'to', 'the', 'library', 'first', 'the', 'real', 'story', 'here', 'is', 'spoc', 'reversion', 'to', 'the', 'barbaric', 'tendencies', 'of', 'his', 'ancestors', 'the', 'warlike', 'vulcans', 'of', '5000', 'years', 'ago', 'this', 'does', 'really', 'make', 'sense', 'except', 'that', 'maybe', 'this', 'time', 'machine', 'is', 'responsible', 'for', 'the', 'change', 'even', 'so', 'spock', 'mccoy', 'were', 'prepared', 'by', 'atoz', 'oh', 'well', 'it', 'also', 'seems', 'to', 'me', 'spock', 'was', 'affected', 'by', 'the', 'transition', 'almost', 'immediately', 'he', 'mentions', 'being', 'from', 'millions', 'of', 'light', 'years', 'away', 'instead', 'of', 'the', 'correct', 'hundreds', 'or', 'thousands', 'a', 'gross', 'error', 'for', 'a', 'logical', 'vulcan', 'in', 'any', 'case', 'spock', 'really', 'shows', 'his', 'nasty', 'side', 'here', 'forget', 'day', 'of', 'the', 'dove', 'and', 'remember', 'this', 'side', 'of', 'paradise', 'mccoy', 'quickly', 'finds', 'out', 'that', 'his', 'vulcan', 'buddy', 'will', 'not', 'stand', 'for', 'any', 'of', 'his', 'usual', 'baiting', 'and', 'nearly', 'gets', 'his', 'face', 'rearranged', 'spock', 'also', 'gets', 'it', 'on', 'with', 'zarabeth', 'a', 'comely', 'female', 'who', 'had', 'been', 'exiled', 'to', 'this', 'cold', 'past', 'as', 'punishment', 'a', 'couple', 'of', 'trek', 'novels', 'were', 'written', 'about', 'spoc', 'son', 'the', 'result', 'of', 'this', 'union', 'all', 'these', 'scenes', 'are', 'eye', 'openers', 'a', 'reminder', 'of', 'just', 'how', 'much', 'spock', 'conceals', 'or', 'holds', 'in', 'i', 'also', 'ironic', 'that', 'only', 'a', 'few', 'episodes', 'earlier', 'requiem', 'for', 'methuselah', 'mccoy', 'was', 'pointing', 'out', 'to', 'spock', 'how', 'he', 'would', 'never', 'know', 'the', 'pain', 'of', 'love', 'and', 'now', 'all', 'this', 'happens', 'kirk', 'meanwhile', 'tussles', 'with', 'the', 'elderly', 'atoz', 'who', 'insists', 'that', 'kirk', 'head', 'back', 'to', 'some', 'past', 'era', 'you', 'are', 'evidently', 'a', 'suicidal', 'maniac', 'great', 'stuff', 'from', 'actor', 'wolfe', 'last', 'seen', 'in', 'bread', 'and', 'circuses', 'it', 'all', 'works', 'out', 'in', 'the', 'end', 'but', 'like', 'i', 'mentioned', 'earlier', 'they', 'cut', 'it', 'very', 'close', 'a', 'neat', 'little', 'trek', 'adventure', 'with', 'a', 'definite', 'cosmic', 'slant'], tags=['positive/5163_7.txt'])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PermuteSentences(object):\n",
    "    def __init__(self, sents):\n",
    "        self.sents = sents\n",
    "    def __iter__(self):\n",
    "        shuffled = list(self.sents)\n",
    "        random.shuffle(shuffled)\n",
    "        for sent in shuffled:\n",
    "            yield sent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "permuter = PermuteSentences(sentences)\n",
    "#Training Doc2Vec model\n",
    "model = Doc2Vec(permuter, dm=0, hs=1, vector_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# done with training, free up some memory\n",
    "model.delete_temporary_training_data(keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = proj_root + \"/\" + \"model\"\n",
    "model.save(model_dir + \"/\" + 'reviews.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01350077,  0.08934327, -0.09697033,  0.02471156, -0.18965186,\n",
       "        0.02942901, -0.03993719, -0.18242534,  0.10629763,  0.23757818,\n",
       "        0.03010215, -0.0022863 , -0.22295296, -0.01697446, -0.14168206,\n",
       "       -0.1323943 ,  0.06893007,  0.07598587,  0.06111291,  0.15839736,\n",
       "       -0.31210947, -0.06756169, -0.11877166, -0.2460407 ,  0.09327912,\n",
       "       -0.00296085,  0.17162102,  0.13997607, -0.20364867,  0.09936013,\n",
       "        0.04297823, -0.11172268,  0.1276897 ,  0.14330468,  0.01672968,\n",
       "        0.02624682,  0.15967034, -0.03492855,  0.43552196, -0.22896902,\n",
       "       -0.22452025, -0.32068533, -0.2026356 , -0.19075608,  0.27631646,\n",
       "       -0.08875233,  0.38306233,  0.32971722, -0.25210103, -0.04952441],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test doc2vec model\n",
    "model.infer_vector(extract_words(\"This is really exciting video. Thank you for presenting to us.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7256148]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check model outcomes using sample sentences\n",
    "cosine_similarity(\n",
    "[model.infer_vector(extract_words(\"This is really exciting video. Thank you for presenting to us.\"))],\n",
    "[model.infer_vector(extract_words(\"Exciting video. Keep it coming\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5963397]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(\n",
    "[model.infer_vector(extract_words(\"It is now snowing in New York\"))],\n",
    "[model.infer_vector(extract_words(\"Golden State Warriors are champions\"))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
