{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_words(sent):\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub(r'<[^>]+>', ' ', sent) #strip html tags\n",
    "    sent = re.sub(r'(\\w)\\'(\\w)', '\\1\\2', sent) # remoce apostrophes\n",
    "    sent = re.sub(r'\\W',' ', sent) # remove punctuation\n",
    "    sent = re.sub(r'\\s+', ' ', sent) # remove repeated space\n",
    "    sent = sent.strip()\n",
    "    return sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ajshd', 'asda', '1231', 'sd', 'alkdj']\n"
     ]
    }
   ],
   "source": [
    "# test text cleanup block\n",
    "words = 'ajshd asda, + 1231 \"sd + ''alkdj''    '\n",
    "print(extract_words(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load doc to vec model\n",
    "doc2vec_model_name = 'reviews.d2v'\n",
    "model = Doc2Vec.load(doc2vec_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00097921,  0.22152558, -0.02575502, -0.05142282, -0.02964645,\n",
       "       -0.08908243, -0.02033703, -0.13232681,  0.15730113,  0.0219765 ,\n",
       "       -0.10882669, -0.24273105, -0.19880445, -0.04594366,  0.07880894,\n",
       "        0.09912173,  0.12206709,  0.13162981,  0.11471985,  0.22692327,\n",
       "       -0.34206438,  0.0351825 ,  0.03957016, -0.09352863,  0.07131914,\n",
       "        0.1587591 ,  0.08153705,  0.14199504,  0.05146929,  0.06841698,\n",
       "        0.31849444, -0.05760468,  0.14546221, -0.15244794, -0.10526983,\n",
       "        0.15762284,  0.13843904, -0.08263648,  0.40170503, -0.15400773,\n",
       "       -0.2481152 , -0.27001816, -0.24904737, -0.10606316,  0.20778543,\n",
       "       -0.03677601,  0.07447217, -0.03276308, -0.0848314 ,  0.15688238],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector(extract_words(\"This is very bad video. I don't like it\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.727353]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(\n",
    "[model.infer_vector(extract_words(\"This is very bad video. I don't like it\"))],\n",
    "[model.infer_vector(extract_words(\"video sucks.\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44124275]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(\n",
    "[model.infer_vector(extract_words(\"It is now snowing in New York\"))],\n",
    "[model.infer_vector(extract_words(\"I feel sick. Dont feel like going to school\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "sentvecs = []\n",
    "sentiments = []\n",
    "# download test dataset : https://www.kaggle.com/rahulin05/sentiment-labelled-sentences-data-set?login=true\n",
    "for fname in [\"yelp\", \"amazon_cells\", \"imdb\"]:\n",
    "    with open(\"data/sentiment-labelled-sentences-data-set/%s_labelled.txt\" % fname, encoding='UTF-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line_split = line.strip().split('\\t')\n",
    "            sentences.append(line_split[0])\n",
    "            words = extract_words(line_split[0])\n",
    "            sentvecs.append(model.infer_vector(words, steps=10)) # create a vector for this document\n",
    "            sentiments.append(int(line_split[1]))\n",
    "# shuffle sentences, sentvecs, sentiments together\n",
    "combined = list(zip(sentences, sentvecs, sentiments))\n",
    "random.shuffle(combined)\n",
    "sentences, sentvecs, sentiments = zip(*combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Never heard of any of them except Cole who was totally unbelievable in the part.  ',\n",
       " 'We are sending it back.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.04064234,  0.12417678,  0.11082955,  0.20703144, -0.06779366,\n",
       "        -0.05296742,  0.04924719, -0.53294235,  0.33222738,  0.12901679,\n",
       "         0.5557371 , -0.49287882, -0.17895997,  0.08885325, -0.06590942,\n",
       "         0.04583032,  0.13890967,  0.49246433, -0.11303261,  0.10337923,\n",
       "         0.20695312,  0.11026672, -0.38203767, -0.5026883 , -0.14516264,\n",
       "         0.23198043,  0.32230964,  0.671394  ,  0.35248262,  0.135847  ,\n",
       "        -0.15433632,  0.06664702,  0.13038336, -0.0049192 , -0.4993708 ,\n",
       "        -0.3800676 ,  0.05269672, -0.24363501,  0.4880163 ,  0.07636135,\n",
       "        -0.49558273, -0.4762459 ,  0.27767465, -0.32609206, -0.20652016,\n",
       "        -0.18339564,  0.02523056, -0.11994597, -0.53938174,  0.45793116],\n",
       "       dtype=float32),\n",
       " array([ 0.1093405 ,  0.13401023, -0.09033331,  0.20825808,  0.02629481,\n",
       "        -0.09775223, -0.09246983, -0.1619282 , -0.08687596,  0.228578  ,\n",
       "         0.27323732, -0.2287021 , -0.29045078, -0.08940927,  0.10642046,\n",
       "         0.2064396 ,  0.09944682, -0.03296569, -0.28279784,  0.1617864 ,\n",
       "        -0.26717055,  0.00424907, -0.01125282, -0.39553624,  0.01246449,\n",
       "        -0.04814537,  0.32325825,  0.0650074 , -0.3516811 ,  0.2933525 ,\n",
       "        -0.17261541,  0.17550236,  0.10956717, -0.02338804, -0.03534603,\n",
       "         0.04858631,  0.26517388, -0.07081066,  0.26537317,  0.1252901 ,\n",
       "         0.04654127, -0.16839729, -0.17498724, -0.01806881,  0.30482766,\n",
       "        -0.0291017 ,  0.11761703,  0.25774315, -0.21439557,  0.2755161 ],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentvecs[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Build a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfkn = KNeighborsClassifier(n_neighbors=9)\n",
    "clfrf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg : 0.7863333333333333 std : 0.01279756921363497\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clfkn, sentvecs, sentiments, cv =5)\n",
    "print(\"avg : {0} std : {1}\".format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg : 0.729 std : 0.012046207333061738\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clfrf, sentvecs, sentiments, cv =5)\n",
    "print(\"avg : {0} std : {1}\".format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model and save it for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=9, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "clfkn.fit(sentvecs, sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "clfrf.fit(sentvecs, sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred : [0 0 1 0 0 1 0 0 1 0]\n",
      "y_test : (0, 0, 1, 0, 0, 1, 0, 0, 1, 0)\n"
     ]
    }
   ],
   "source": [
    "y_pred = clfkn.predict(sentvecs[0:10])\n",
    "print(\"y_pred : {0}\".format(y_pred))\n",
    "y_test = sentiments[0:10]\n",
    "print(\"y_test : {0}\".format(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy : {0}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Metrics : \n",
      "[[7 0]\n",
      " [0 3]]\n"
     ]
    }
   ],
   "source": [
    "cf = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Metrics : \\n{0}\".format(cf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save the model\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doc2vec_kn.model']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'doc2vec_kn.model'\n",
    "joblib.dump(clfkn, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doc2vec_rf.model']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'doc2vec_rf.model'\n",
    "joblib.dump(clfrf, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "model_name = 'doc2vec_rf.sav'\n",
    "pickle.dump(clfrf, open(model_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(model_name, 'rb'))\n",
    "result = loaded_model.score(sentvecs[0:10], sentiments[0:10])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag-of-Words model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "pipeline = make_pipeline(CountVectorizer(), TfidfTransformer(), RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg : 0.7470000000000001 std : 0.01596524001977072\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(pipeline, sentences, sentiments, cv =5)\n",
    "print(\"avg : {0} std : {1}\".format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bag-of-Words and Doc2Vec performance seems to be close. However, if we have lot more training example Doc2Vec works better"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
