{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_words(sent):\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub(r'<[^>]+>', ' ', sent) #strip html tags\n",
    "    sent = re.sub(r'(\\w)\\'(\\w)', '\\1\\2', sent) # remoce apostrophes\n",
    "    sent = re.sub(r'\\W',' ', sent) # remove punctuation\n",
    "    sent = re.sub(r'\\s+', ' ', sent) # remove repeated space\n",
    "    sent = sent.strip()\n",
    "    return sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chirstmas', 'time', 'santa', 'time', '2018']\n"
     ]
    }
   ],
   "source": [
    "# test text cleanup block\n",
    "words = 'Chirstmas time...Santa time, 2018  + ''//''    '\n",
    "print(extract_words(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proj_root = os.path.dirname(os.getcwd())\n",
    "#print(\"Current working dir : {0}\".format(proj_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dir = proj_root + \"/\" + \"model\"\n",
    "doc2vec_model = model_dir + \"/\" + 'comments.d2v'\n",
    "\n",
    "#Load doc to vec model\n",
    "model = Doc2Vec.load(doc2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10651165,  0.08706113, -0.03325614, -0.09894038,  0.03064333,\n",
       "       -0.019945  ,  0.03584543, -0.20947409,  0.05308295,  0.07270622,\n",
       "        0.00521208, -0.08042626, -0.0354125 , -0.03602903, -0.05012208,\n",
       "        0.04042677,  0.00169689,  0.06674345, -0.00993093,  0.09504773,\n",
       "       -0.05088811, -0.03965221, -0.11067194,  0.04205915,  0.0532117 ,\n",
       "       -0.03844444,  0.08866784,  0.05826956, -0.07038295,  0.00540849,\n",
       "        0.07919995, -0.09133605,  0.07977698,  0.01242394, -0.03988631,\n",
       "        0.07748676, -0.05657875, -0.03706421,  0.13353555, -0.02652852,\n",
       "       -0.02558131, -0.22499587,  0.02591169, -0.10143113,  0.12842202,\n",
       "        0.00634575,  0.06762049,  0.07074005, -0.044052  ,  0.04046854],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector(extract_words(\"This is awesome...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load labeled data\n",
    "#Trying to use pandas\n",
    "labeled_spam_subdir = 'traindata/labeled-spam-data'\n",
    "labeled_spam_dir = proj_root + \"/\" + labeled_spam_subdir\n",
    "               \n",
    "data = pd.concat([pd.read_csv(labeled_spam_dir + \"/\" + \"Youtube01-Psy.csv\"), \n",
    "               pd.read_csv(labeled_spam_dir + \"/\" + \"Youtube02-KatyPerry.csv\"),\n",
    "              pd.read_csv(labeled_spam_dir + \"/\" + \"Youtube03-LMFAO.csv\"),\n",
    "              pd.read_csv(labeled_spam_dir + \"/\" + \"Youtube04-Eminem.csv\"),\n",
    "              pd.read_csv(labeled_spam_dir + \"/\" + \"Youtube05-Shakira.csv\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Huh, anyway check out this you[tube] channel: ...\n",
       "1    Hey guys check out my new channel and our firs...\n",
       "Name: CONTENT, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['CONTENT'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['WORDS'] = data['CONTENT'].map(lambda x: extract_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [huh, anyway, check, out, this, you, tube, cha...\n",
       "1    [hey, guys, check, out, my, new, channel, and,...\n",
       "Name: WORDS, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['WORDS'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = data['CONTENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['SENTVECS'] = data['WORDS'].map(lambda x: model.infer_vector(x, steps=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [0.16520166, 0.20271881, 0.1698854, 0.1000886,...\n",
       "1    [0.09364169, 0.09967823, -0.21531972, 0.103081...\n",
       "2    [-0.029987874, 0.02582819, 0.18829851, -0.3173...\n",
       "3    [0.2704871, 0.07241474, 0.17938806, -0.2955648...\n",
       "4    [0.13608585, 0.04917111, 0.07191952, 0.082964,...\n",
       "Name: SENTVECS, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['SENTVECS'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentvecs = data['SENTVECS'].tolist()\n",
    "labels = data['CLASS'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Build a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfkn = KNeighborsClassifier(n_neighbors=9)\n",
    "clfrf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg : 0.8936909546427266 std : 0.030292858107524863\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clfkn, sentvecs, labels, cv =5)\n",
    "print(\"avg : {0} std : {1}\".format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg : 0.8783639542773631 std : 0.04028561450393364\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clfrf, sentvecs, labels, cv =5)\n",
    "print(\"avg : {0} std : {1}\".format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model and save it for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=9, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "clfkn.fit(sentvecs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "clfrf.fit(sentvecs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred : [1 1 1 1 1 1 1 1 1 1]\n",
      "y_test : [1, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clfkn.predict(sentvecs[0:10])\n",
    "print(\"y_pred : {0}\".format(y_pred))\n",
    "y_test = labels[0:10]\n",
    "print(\"y_test : {0}\".format(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save the model\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dir = proj_root + \"/\" + \"model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/dbiswas/Documents/Malabika/MS/Fall2018/social_media_mining/project/comments_analysis/model/doc2vec_spam_kn.model']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = model_dir + \"/\" + 'doc2vec_spam_kn.model'\n",
    "joblib.dump(clfkn, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/dbiswas/Documents/Malabika/MS/Fall2018/social_media_mining/project/comments_analysis/model/doc2vec_spam_rf.model']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = model_dir + \"/\" + 'doc2vec_spam_rf.model'\n",
    "joblib.dump(clfrf, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "model_name = model_dir + \"/\" + 'doc2vec_spam_rf.sav'\n",
    "pickle.dump(clfrf, open(model_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag-of-Words model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "pipeline = make_pipeline(CountVectorizer(), TfidfTransformer(), RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg : 0.9151312698992641 std : 0.01103018082373847\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(pipeline, sentences, labels, cv =5)\n",
    "print(\"avg : {0} std : {1}\".format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bag-of-Words and Doc2Vec performance seems to be close. However, if we have lot more training example Doc2Vec works better"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
