{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model and make prediction\n",
    "##### Input Args:\n",
    "\n",
    "doc_dir = directory where the file with comments to be predicted\n",
    "\n",
    "doc2vec_model_name = doc2vec model already trained\n",
    "\n",
    "trained_clf = Trained classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_words(sent):\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub(r'<[^>]+>', ' ', sent) #strip html tags\n",
    "    sent = re.sub(r'(\\w)\\'(\\w)', '\\1\\2', sent) # remoce apostrophes\n",
    "    sent = re.sub(r'\\W',' ', sent) # remove punctuation\n",
    "    sent = re.sub(r'\\s+', ' ', sent) # remove repeated space\n",
    "    sent = sent.strip()\n",
    "    return sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load doc to vec model\n",
    "doc2vec_model_name = 'reviews.d2v'\n",
    "doc2vec_model = Doc2Vec.load(doc2vec_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6934385]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test the trained doc2vec model\n",
    "cosine_similarity(\n",
    "[doc2vec_model.infer_vector(extract_words(\"This is very bad video. I don't like it\"))],\n",
    "[doc2vec_model.infer_vector(extract_words(\"video sucks.\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converts the comments to vector using doc2vec model trianed earlier\n",
    "def get_doc2vec(model, doc_dir,comment_text_pos=0):\n",
    "    sentences = []\n",
    "    sentvecs = []\n",
    "    \n",
    "    for fname in sorted(os.listdir(doc_dir)):\n",
    "        with open(doc_dir + \"/\" + fname, encoding='UTF-8') as f:\n",
    "            print(\"files being read : {0}\".format(fname))\n",
    "            for i, line in enumerate(f):\n",
    "                line_split = line.strip().split('\\t')\n",
    "                sentences.append(line_split[comment_text_pos])\n",
    "                words = extract_words(line_split[comment_text_pos])\n",
    "                sentvecs.append(model.infer_vector(words, steps=10)) # create a vector for this document\n",
    "        \n",
    "    #sentences, sentvecs together\n",
    "    return sentences, sentvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files being read : yelp_labelled.txt\n"
     ]
    }
   ],
   "source": [
    "doc_dir = \"data/Test\"\n",
    "sentences, sentvecs = get_doc2vec(doc2vec_model, doc_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences : 1000\n"
     ]
    }
   ],
   "source": [
    "print(\"sentences : {0}\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wow... Loved this place.',\n",
       " 'Crust is not good.',\n",
       " 'Not tasty and the texture was just nasty.',\n",
       " 'Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.',\n",
       " 'The selection on the menu was great and so were the prices.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.24903746, -0.18018925,  0.05749322, -0.10560571, -0.33776838,\n",
       "         0.16579776,  0.17553787,  0.17636697, -0.07393146, -0.14072871,\n",
       "         0.09156888,  0.02022403, -0.09713821,  0.13383433, -0.17543945,\n",
       "         0.15709971, -0.03591085,  0.15174581,  0.13046418, -0.01549064,\n",
       "         0.13690972,  0.2751294 , -0.05023609,  0.15681918, -0.1916324 ,\n",
       "         0.2513422 , -0.23718649,  0.10328327, -0.14861111, -0.03345313,\n",
       "        -0.08967361,  0.1354041 , -0.18289335, -0.15940091, -0.2080593 ,\n",
       "         0.50230086,  0.30272844,  0.18101959,  0.22926323,  0.02978012,\n",
       "         0.15447672, -0.04651478, -0.01549169,  0.23443107,  0.00571839,\n",
       "         0.20836784, -0.07697911,  0.03664642,  0.38524595, -0.04609485],\n",
       "       dtype=float32),\n",
       " array([-0.07684638,  0.09519014,  0.05153202, -0.14022346,  0.08552711,\n",
       "         0.26134858,  0.16231316,  0.05601692,  0.08603787,  0.00941966,\n",
       "        -0.04827184, -0.14852993, -0.05336305,  0.10257004,  0.11232131,\n",
       "         0.30058882,  0.02580031,  0.11591643,  0.25071204, -0.1314004 ,\n",
       "         0.09183832,  0.07196615, -0.03542633,  0.15176897,  0.01989593,\n",
       "        -0.24769802, -0.0955357 , -0.05856776, -0.05326962, -0.04778594,\n",
       "         0.16818011, -0.03673558, -0.15405674, -0.06182231, -0.26726955,\n",
       "         0.29855955,  0.1511405 ,  0.12941115,  0.3048277 , -0.16427027,\n",
       "        -0.09822722, -0.13527113, -0.02004298,  0.01868692,  0.21804956,\n",
       "        -0.11621288,  0.20300502,  0.25920567, -0.00123776, -0.08848175],\n",
       "       dtype=float32),\n",
       " array([-0.1348309 , -0.10656536,  0.23245782, -0.12797152, -0.16691574,\n",
       "         0.2278628 ,  0.22193393,  0.18544067, -0.15509547,  0.39282876,\n",
       "        -0.15729637, -0.38226673, -0.04517556, -0.01165881, -0.06911272,\n",
       "         0.5709329 ,  0.13732448,  0.08907311,  0.38170642,  0.02692717,\n",
       "         0.07309069,  0.11534938,  0.01607141, -0.02507876, -0.00742774,\n",
       "         0.09948937, -0.10925113,  0.13457103,  0.01715597, -0.18061578,\n",
       "         0.02179922,  0.24528146, -0.00653533, -0.11827317, -0.0714349 ,\n",
       "         0.48337066,  0.22004631,  0.23132807,  0.10410496, -0.04484393,\n",
       "        -0.14159663, -0.04381223, -0.04292365,  0.15892054, -0.00401263,\n",
       "        -0.09059312,  0.06794377,  0.15177503,  0.13081819, -0.13100381],\n",
       "       dtype=float32),\n",
       " array([-0.5304394 , -0.07591768,  0.50987446,  0.09226865, -0.4591743 ,\n",
       "         0.2345247 ,  0.24588858, -0.31156883,  0.1052784 , -0.08880454,\n",
       "         0.08747316,  0.05552022,  0.2570318 , -0.07497649,  0.04977244,\n",
       "         0.66279906, -0.6273471 , -0.08879999,  0.09519529,  0.14287034,\n",
       "         0.36712414,  0.2552036 ,  0.46825716,  0.02139322, -0.3384516 ,\n",
       "        -0.04558654, -0.28345072, -0.32125285, -0.05261872,  0.25958794,\n",
       "         0.1597509 , -0.18370952, -0.09107062, -0.23866244, -0.36330193,\n",
       "         0.14857243,  0.7602326 ,  0.15804453, -0.04764035, -0.21293257,\n",
       "        -0.10159162, -0.22575176,  0.00282783,  0.57341784, -0.0188138 ,\n",
       "         0.12265074,  0.44511947,  0.42159662,  0.11245368, -0.05412964],\n",
       "       dtype=float32),\n",
       " array([-0.27631205,  0.00578328,  0.03013609, -0.14530288, -0.3129059 ,\n",
       "         0.2550461 ,  0.12369445, -0.22684775, -0.40007794,  0.04027402,\n",
       "        -0.17177223, -0.08775657, -0.16836976,  0.30156404,  0.1540009 ,\n",
       "         0.24760714, -0.01428091,  0.24040152, -0.04388292,  0.16181228,\n",
       "         0.19474399,  0.48522994, -0.2525669 ,  0.05625811,  0.08108898,\n",
       "        -0.12293223,  0.22491086,  0.03119251,  0.30158973,  0.03300885,\n",
       "         0.2420254 ,  0.22264953, -0.50219417, -0.32999396, -0.35235763,\n",
       "         0.33262235,  0.49939167,  0.11492129,  0.5859722 , -0.02421442,\n",
       "        -0.11857307, -0.176298  ,  0.05355871,  0.27225223,  0.30604187,\n",
       "         0.0725204 ,  0.2261584 , -0.0078648 ,  0.25794685,  0.05431503],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentvecs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "trained_clf = 'doc2vec_rf.model'\n",
    "loaded_clf = joblib.load(trained_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 1\n",
      " 0 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
      " 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1\n",
      " 0 1 1 1 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1 1 0 0\n",
      " 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 0 0 1\n",
      " 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0\n",
      " 0 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0\n",
      " 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1\n",
      " 0 1 1 0 0 1 0 0 0 1 1 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 0\n",
      " 0 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 1\n",
      " 1 1 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 0 0 0 0 1 0\n",
      " 1 1 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 0\n",
      " 1 0 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 1\n",
      " 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 0 0\n",
      " 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 0 1 1\n",
      " 1 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 1 0 0\n",
      " 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1\n",
      " 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 0 0 1\n",
      " 1 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1\n",
      " 1 0 0 0 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1\n",
      " 0 0 0 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1\n",
      " 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1\n",
      " 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0\n",
      " 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "y_pred = loaded_clf.predict(sentvecs)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trained_clf = 'doc2vec_kn.model'\n",
    "loaded_clf = joblib.load(trained_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 0\n",
      " 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0\n",
      " 0 1 0 1 1 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1\n",
      " 0 1 1 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1\n",
      " 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1\n",
      " 1 0 0 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0\n",
      " 0 0 0 1 0 1 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1 0 0\n",
      " 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1\n",
      " 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 1 1 1\n",
      " 0 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 1 1\n",
      " 0 0 1 1 0 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1\n",
      " 1 1 0 0 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0\n",
      " 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0\n",
      " 0 1 1 0 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 1\n",
      " 0 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 0\n",
      " 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 0\n",
      " 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 1 0 1 1\n",
      " 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0\n",
      " 1 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 1\n",
      " 1 0 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 1 1 1 0 0 0\n",
      " 1 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1\n",
      " 0 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 0\n",
      " 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "y_pred = loaded_clf.predict(sentvecs)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
